#  Modular ETL Pipeline â€” Azure Blob â†’ Snowflake & Snowflake Parallel Ingestion!!!

This repository contains a robust ETL framework that ingests:

- **Azure Blob Storage â†’ Snowflake (CSV ingestion)**
- **Snowflake Source â†’ Snowflake Target migration**

with:

âœ” L1 / TEMP â†’ L2 Final promotion  
âœ” Slack summary reporting  
âœ” Ingestion logging  
âœ” Parallel execution for faster pipelines  
âœ” Fully environment-driven configuration  

---

## âœ¨ Key Features

| Feature | Description |
|---|---|
| Azure CSV ingestion | Loads CSVs â†’ Pandas â†’ Snowflake L1 â†’ promotes to L2 |
| Snowflake â†’ Snowflake sync | Auto-detects source tables and ingests all in parallel |
| L1 / L2 ingestion model | Prevents corruption + ensures data lineage visibility |
| Slack monitoring | Sends completion summary + mismatch alerts |
| Ingestion logging table | Tracks row counts, timestamp, layer, and source |
| Modular + scalable | Clean structure for adding more connectors or DQ checks |

---

## ğŸ“‚ Repository Structure

```
etl-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ azure_ingest.py
â”‚   â”œâ”€â”€ snowflake_ingest.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ connections.py
â”‚   â”œâ”€â”€ slack_utils.py
â”‚   â”œâ”€â”€ logging_utils.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ helpers.py (optional placeholder)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  ETL Process Flow

### Azure Blob â†’ Snowflake

1. Discover CSVs or load from AZURE_FILES list  
2. Convert to DataFrame  
3. Insert into `<TABLE>_TEMP` (L1)  
4. Promote into `<TABLE>` (L2)  
5. Record ingestion metrics  
6. Slack + log output  

### Snowflake â†’ Snowflake (Parallel)

| Stage | Operation |
|---|---|
| Table scan | Reads all tables from INFORMATION_SCHEMA |
| Parallel execution | ThreadPool spawns workers per table |
| Load to L1 | `_TEMP` staging per table |
| Promote to L2 | Replace existing final table after null checks|
| Logging | Stores record counts for traceability |

---

## ğŸ” .env Configuration (Required)

```
SLACK_TOKEN=
SLACK_CHANNEL=#etl-alerts

AZURE_CONNECTION_STRING=
AZURE_CONTAINER=
AZURE_FILES=      # comma separated or empty = autodetect

SF_SOURCE_USER=
SF_SOURCE_ACCOUNT=
SF_SOURCE_PRIVATE_KEY=
SF_SOURCE_WAREHOUSE=
SF_SOURCE_DATABASE=
SF_SOURCE_SCHEMA=
SF_SOURCE_ROLE=

SF_TARGET_USER=
SF_TARGET_ACCOUNT=
SF_TARGET_PRIVATE_KEY=
SF_TARGET_WAREHOUSE=
SF_TARGET_DATABASE=
SF_TARGET_SCHEMA=
SF_TARGET_ROLE=

LOG_TABLE=ETL_DB.ETL_SCHEMA.INGESTION_LOGS
```

Copy `.env.example` â†’ `.env` and populate.

---

## ğŸ“¦ Installation

```
pip install -r requirements.txt
```

---

## â–¶ï¸ Run the Pipeline

```
python src/main.py
```

---

## ğŸ“¡ Slack Summary Output Example

```
ETL SUMMARY â€” 12 Feb 2025 09:48PM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TABLE_1         SRC=10XXXX  FINAL=10XXX   âœ“
TABLE_2         SRC=YYYY   FINAL=YYYY    âœ“
TABLE_3         SRC=0      FINAL=0       skipped
TABLE_4         SRC=1500   FINAL=1487    âš  mismatch
```

---

## ğŸ›  Requirements

```
pandas
python-dotenv
requests
snowflake-connector-python
azure-storage-blob
sqlalchemy
```

---

## ğŸ”¥ Optional Enhancements

| Suggestion | Benefit |
|---|---|
| Add Data Quality layer | threshold rules, schema drift alerts |
| Enable CDC / MERGE logic | Avoids full reload, incremental ingest supported |
| Convert to Airflow / Prefect DAG | Scheduling + monitoring + retries |
| Add AWS S3/GCS handlers | Multi-cloud ingestion architecture |

---

## ğŸ§¾ .gitignore Recommended

```
.env
*.p8
__pycache__/
*.log
```

---

## ğŸ“„ License

MIT License (recommended for open-source use)

```
MIT License
Copyright (c) 2025
Permission is hereby granted, free of charge, to any person obtaining a copy
...
```

---

## ğŸ¤ Contributions

PRs, discussions, and extensions welcome! Add new connectors, DQ validation modules, or orchestration integrations and help grow the framework. Credit Would be Appreciated
